{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from corpus import ConllCorpusReaderX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "result = []\n",
    "\n",
    "for file in os.listdir('./bsnlp_dataset/eu_annot/'):\n",
    "    number = int(file.split('.')[0])\n",
    "    with open('./bsnlp_dataset/eu_annot/' + file, encoding='utf-8') as annot_file:\n",
    "        annot = annot_file.readlines()[1:]\n",
    "        annot = [simple_word_tokenize(el.strip()) for el in annot]\n",
    "        annot = [el[:int((len(el) - 2)/2)] + [el[-2]] for el in annot]\n",
    "    with open('./bsnlp_dataset/eu/file_' + str(number) + \"_ru.txt\", encoding='utf-8') as text_file:\n",
    "        text = text_file.readlines()[4:]\n",
    "        text = [simple_word_tokenize(line.strip()) for line in text]\n",
    "        text = [el for el in text if el != []]\n",
    "        total_words = []\n",
    "        for el in text:\n",
    "            total_words += el\n",
    "            total_words += ' '\n",
    "        total_words = [el.lower() for el in total_words]\n",
    "        total_ne = ['O' for el in total_words]\n",
    "\n",
    "    for ne in annot:\n",
    "        words_count = len(ne) - 1\n",
    "        for i in range(len(total_words) - words_count - 1):\n",
    "            if ne[:-1] == total_words[i: i + words_count]:\n",
    "                if words_count == 1:\n",
    "                    total_ne[i] = 'S-' + ne[-1]\n",
    "                else:\n",
    "                    total_ne[i] = 'B-' + ne[-1]\n",
    "                    total_ne[i + words_count - 1] = 'E-' + ne[-1]\n",
    "                    for j in range(i + 1, i + words_count - 1):\n",
    "                        total_ne[j] = 'I-' + ne[-1]\n",
    "\n",
    "    for word, ne in zip(total_words, total_ne):\n",
    "        if word != \" \":\n",
    "            result.append(word + ' ' +  ne + '\\n')\n",
    "        else:\n",
    "            result.append('\\n')\n",
    "            \n",
    "with open('./bsnlp_dataset/ec.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(\"-DOCSTART- O\\n\")\n",
    "    for line in result:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for file in os.listdir('./bsnlp_dataset/trump_annot/'):\n",
    "    number = int(file.split('.')[0])\n",
    "    with open('./bsnlp_dataset/trump_annot/' + file, encoding='utf-8') as annot_file:\n",
    "        annot = annot_file.readlines()[1:]\n",
    "        annot = [simple_word_tokenize(el.strip()) for el in annot]\n",
    "        annot = [el[:int((len(el) - 2)/2)] + [el[-2]] for el in annot]\n",
    "    with open('./bsnlp_dataset/trump/file_' + str(number) + \".txt\", encoding='utf-8') as text_file:\n",
    "        text = text_file.readlines()[4:]\n",
    "        text = [simple_word_tokenize(line.strip()) for line in text]\n",
    "        text = [el for el in text if el != []]\n",
    "        total_words = []\n",
    "        for el in text:\n",
    "            total_words += el\n",
    "            total_words += ' '\n",
    "        total_words = [el.lower() for el in total_words]\n",
    "        total_ne = ['O' for el in total_words]\n",
    "\n",
    "    for ne in annot:\n",
    "        words_count = len(ne) - 1\n",
    "        for i in range(len(total_words) - words_count - 1):\n",
    "            if ne[:-1] == total_words[i: i + words_count]:\n",
    "                if words_count == 1:\n",
    "                    total_ne[i] = 'S-' + ne[-1]\n",
    "                else:\n",
    "                    total_ne[i] = 'B-' + ne[-1]\n",
    "                    total_ne[i + words_count - 1] = 'E-' + ne[-1]\n",
    "                    for j in range(i + 1, i + words_count - 1):\n",
    "                        total_ne[j] = 'I-' + ne[-1]\n",
    "\n",
    "    for word, ne in zip(total_words, total_ne):\n",
    "        if word != \" \":\n",
    "            result.append(word + ' ' +  ne + '\\n')\n",
    "        else:\n",
    "            result.append('\\n')\n",
    "            \n",
    "with open('./bsnlp_dataset/trump.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write(\"-DOCSTART- O\\n\")\n",
    "    for line in result:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_dataset = ConllCorpusReaderX('./bsnlp_dataset/', \n",
    "                              fileids='trump.txt', \n",
    "                              columntypes=('words', 'ne'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eu_dataset = ConllCorpusReaderX('./bsnlp_dataset/', \n",
    "                              fileids='eu.txt', \n",
    "                              columntypes=('words', 'ne'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('трамп', 'S-PER'), ('разошелся', 'O'), ('во', 'O'), ('мнении', 'O'), ('с', 'O'), ('мишель', 'B-PER'), ('обамой', 'E-PER'), ('о', 'O'), ('судьбе', 'O'), ('сша', 'S-LOC')]\n"
     ]
    }
   ],
   "source": [
    "print(trump_dataset.get_ne()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['трамп', 'разошелся', 'во', 'мнении', 'с', 'мишель', ...]\n",
      "5474\n"
     ]
    }
   ],
   "source": [
    "print(trump_dataset.words())\n",
    "print(len(trump_dataset.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from generator import Generator\n",
    "\n",
    "words = [[el] for el in trump_dataset.words()[:4000]]\n",
    "gen = Generator(column_types=['word'], context_len=1)\n",
    "X = gen.generate(words, path='./trump_dataset.npy')\n",
    "y = np.array([el[1] for el in trump_dataset.get_ne()[:4000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X[:3000], y[:3000])\n",
    "y_pred = clf.predict(X[3000:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "total_a = 0\n",
    "total_b = 0\n",
    "for a, b in zip(y_pred, y[3000:4000]):\n",
    "    if a != 'O':\n",
    "        total_a += 1\n",
    "    if b != 'O':\n",
    "        total_b += 1    \n",
    "    if a != 'O' and a == b:\n",
    "        right += 1\n",
    "accuracy = right/total_a\n",
    "recall = right/total_b\n",
    "f = 2 * accuracy * recall /(accuracy + recall)\n",
    "print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
