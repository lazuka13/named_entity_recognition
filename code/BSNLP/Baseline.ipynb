{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from generator import Generator\n",
    "from corpus import ConllCorpusReaderX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EC_PATH = \"./bsnlp_ec.npz\"\n",
    "TRUMP_PATH = \"./bsnlp_trump.npz\"\n",
    "TRAINSET_PATH = \"./factrueval_trainset.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_dataset = ConllCorpusReaderX('./bsnlp_dataset/', \n",
    "                              fileids='trump.txt', \n",
    "                              columntypes=('words', 'ne'))\n",
    "\n",
    "eu_dataset = ConllCorpusReaderX('./bsnlp_dataset/', \n",
    "                              fileids='ec.txt', \n",
    "                              columntypes=('words', 'ne'))\n",
    "\n",
    "factrueval_devset = ConllCorpusReaderX('../FactRuEval/factrueval2016_dataset/', \n",
    "                                        fileids='devset.txt', \n",
    "                                        columntypes=['words', 'offset', 'len', 'ne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(column_types=['WORD'], context_len=2)\n",
    "\n",
    "def get_class(el):\n",
    "    if el == \n",
    "\n",
    "Y_train = [el[1] for el in factrueval_devset.get_ne()]\n",
    "Y_train = [get_class(el) for el in Y_train]\n",
    "\n",
    "Y_test_eu = [el[1] for el in eu_dataset.get_ne()]\n",
    "Y_test_trump = [el[1] for el in trump_dataset.get_ne()] \n",
    "\n",
    "X_train = gen.fit_transform([[el] for el in factrueval_devset.words()], \n",
    "                            Y_train, \n",
    "                            path=TRAINSET_PATH)\n",
    "X_test_eu = gen.transform([[el] for el in eu_dataset.words()], \n",
    "                       path=EC_PATH)\n",
    "X_test_trump = gen.transform([[el] for el in trump_dataset.words()], \n",
    "                       path=TRUMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Избавляет данные от случаев O : O #\n",
    "def clean(Y_pred, Y_test):\n",
    "    Y_pred = np.array(Y_pred)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    Y_pred_i = np.array([Y_pred != 'O'])\n",
    "    Y_test_i = np.array([Y_test != 'O'])\n",
    "\n",
    "    indexes = (Y_pred_i | Y_test_i).reshape(Y_pred.shape)\n",
    "\n",
    "    Y_pred_fixed = Y_pred[indexes]\n",
    "    Y_test_fixed = Y_test[indexes]\n",
    "    return Y_pred_fixed, Y_test_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_baseline(clf=LogisticRegression()):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test_eu)\n",
    "    Y_pred_c, Y_test_c = clean(Y_pred, Y_test_eu)\n",
    "\n",
    "    def get_el(el):\n",
    "        if el == \"O\":\n",
    "            return el\n",
    "        else:\n",
    "            return el[2:]\n",
    "\n",
    "    Y_pred_c_light = [get_el(el) for el in Y_pred_c]\n",
    "    Y_test_c_light = [get_el(el) for el in Y_test_c]\n",
    "\n",
    "    # Strict evaluation #\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"# Strict evaluation #\")\n",
    "    counter = Counter(Y_test_c)\n",
    "    labels = list(counter.keys())\n",
    "    labels.remove(\"O\")\n",
    "    results = f1_score(Y_test_c, Y_pred_c, average=None, labels=labels)\n",
    "    for a, b in zip(labels, results):\n",
    "        print('F1 for {} == {}, with {} entities'.format(a, b, counter[a]))\n",
    "\n",
    "    print(\"Weighted Score:\", f1_score(Y_test_c, Y_pred_c, average=\"weighted\", labels=list(counter.keys())))    \n",
    "\n",
    "    # Not strict evaluation #    \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"# Not strict evaluation #\")    \n",
    "    light_counter = Counter(Y_test_c_light)\n",
    "    light_labels = list(light_counter.keys())\n",
    "    light_labels.remove(\"O\")\n",
    "    print(light_counter)\n",
    "    light_results = f1_score(Y_test_c_light, Y_pred_c_light, average=None, labels=light_labels)\n",
    "    for a, b in zip(light_labels, light_results):\n",
    "        print('F1 for {} == {}, with {} entities'.format(a, b, light_counter[a]))\n",
    "\n",
    "    print(\"Weighted Score:\", f1_score(Y_test_c_light, Y_pred_c_light, average=\"weighted\", labels=light_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for S-ORG == 0.0, with 296 entities\n",
      "F1 for B-ORG == 0.0, with 215 entities\n",
      "F1 for E-ORG == 0.0, with 196 entities\n",
      "F1 for B-MISC == 0.0, with 42 entities\n",
      "F1 for E-MISC == 0.0, with 32 entities\n",
      "F1 for S-LOC == 0.0, with 148 entities\n",
      "F1 for B-PER == 0.0, with 25 entities\n",
      "F1 for E-PER == 0.0, with 23 entities\n",
      "F1 for S-PER == 0.0, with 20 entities\n",
      "F1 for I-ORG == 0.0, with 93 entities\n",
      "F1 for I-PER == 0.0, with 2 entities\n",
      "F1 for S-MISC == 0.0, with 62 entities\n",
      "F1 for I-MISC == 0.0, with 22 entities\n",
      "F1 for B-LOC == 0.0, with 13 entities\n",
      "F1 for E-LOC == 0.0, with 13 entities\n",
      "F1 for I-LOC == 0.0, with 1 entities\n",
      "Weighted Score: 0.0\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'ORG': 800, 'LOC': 175, 'MISC': 158, 'PER': 70, 'O': 22})\n",
      "F1 for ORG == 0.0, with 800 entities\n",
      "F1 for MISC == 0.0, with 158 entities\n",
      "F1 for LOC == 0.0, with 175 entities\n",
      "F1 for PER == 0.0, with 70 entities\n",
      "Weighted Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for S-ORG == 0.0, with 296 entities\n",
      "F1 for B-ORG == 0.0, with 215 entities\n",
      "F1 for E-ORG == 0.0, with 196 entities\n",
      "F1 for B-MISC == 0.0, with 42 entities\n",
      "F1 for E-MISC == 0.0, with 32 entities\n",
      "F1 for S-LOC == 0.0, with 148 entities\n",
      "F1 for B-PER == 0.0, with 25 entities\n",
      "F1 for E-PER == 0.0, with 23 entities\n",
      "F1 for S-PER == 0.0, with 20 entities\n",
      "F1 for I-ORG == 0.0, with 93 entities\n",
      "F1 for I-PER == 0.0, with 2 entities\n",
      "F1 for S-MISC == 0.0, with 62 entities\n",
      "F1 for I-MISC == 0.0, with 22 entities\n",
      "F1 for B-LOC == 0.0, with 13 entities\n",
      "F1 for E-LOC == 0.0, with 13 entities\n",
      "F1 for I-LOC == 0.0, with 1 entities\n",
      "Weighted Score: 0.0\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'ORG': 800, 'LOC': 175, 'MISC': 158, 'PER': 70, 'O': 28})\n",
      "F1 for ORG == 0.0, with 800 entities\n",
      "F1 for MISC == 0.0, with 158 entities\n",
      "F1 for LOC == 0.0, with 175 entities\n",
      "F1 for PER == 0.0, with 70 entities\n",
      "Weighted Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "run_baseline(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for S-ORG == 0.0, with 296 entities\n",
      "F1 for B-ORG == 0.0, with 215 entities\n",
      "F1 for E-ORG == 0.0, with 196 entities\n",
      "F1 for B-MISC == 0.0, with 42 entities\n",
      "F1 for E-MISC == 0.0, with 32 entities\n",
      "F1 for S-LOC == 0.0, with 148 entities\n",
      "F1 for B-PER == 0.0, with 25 entities\n",
      "F1 for E-PER == 0.0, with 23 entities\n",
      "F1 for S-PER == 0.0, with 20 entities\n",
      "F1 for I-ORG == 0.0, with 93 entities\n",
      "F1 for I-PER == 0.0, with 2 entities\n",
      "F1 for S-MISC == 0.0, with 62 entities\n",
      "F1 for I-MISC == 0.0, with 22 entities\n",
      "F1 for B-LOC == 0.0, with 13 entities\n",
      "F1 for E-LOC == 0.0, with 13 entities\n",
      "F1 for I-LOC == 0.0, with 1 entities\n",
      "Weighted Score: 0.0\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'ORG': 800, 'LOC': 175, 'MISC': 158, 'O': 99, 'PER': 70})\n",
      "F1 for ORG == 0.0, with 800 entities\n",
      "F1 for MISC == 0.0, with 158 entities\n",
      "F1 for LOC == 0.0, with 175 entities\n",
      "F1 for PER == 0.0, with 70 entities\n",
      "Weighted Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "run_baseline(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Strict evaluation #\n",
      "F1 for S-ORG == 0.0, with 296 entities\n",
      "F1 for B-ORG == 0.0, with 215 entities\n",
      "F1 for E-ORG == 0.0, with 196 entities\n",
      "F1 for B-MISC == 0.0, with 42 entities\n",
      "F1 for E-MISC == 0.0, with 32 entities\n",
      "F1 for S-LOC == 0.0, with 148 entities\n",
      "F1 for B-PER == 0.0, with 25 entities\n",
      "F1 for E-PER == 0.0, with 23 entities\n",
      "F1 for S-PER == 0.0, with 20 entities\n",
      "F1 for I-ORG == 0.0, with 93 entities\n",
      "F1 for I-PER == 0.0, with 2 entities\n",
      "F1 for S-MISC == 0.0, with 62 entities\n",
      "F1 for I-MISC == 0.0, with 22 entities\n",
      "F1 for B-LOC == 0.0, with 13 entities\n",
      "F1 for E-LOC == 0.0, with 13 entities\n",
      "F1 for I-LOC == 0.0, with 1 entities\n",
      "Weighted Score: 0.0\n",
      "\n",
      "# Not strict evaluation #\n",
      "Counter({'ORG': 800, 'LOC': 175, 'MISC': 158, 'PER': 70, 'O': 65})\n",
      "F1 for ORG == 0.0, with 800 entities\n",
      "F1 for MISC == 0.0, with 158 entities\n",
      "F1 for LOC == 0.0, with 175 entities\n",
      "F1 for PER == 0.0, with 70 entities\n",
      "Weighted Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "run_baseline(GradientBoostingClassifier())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
