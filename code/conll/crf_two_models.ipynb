{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reader\n",
    "import scorer\n",
    "import utils\n",
    "import classifiers.sequence_classifier as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.model_selection import cross_val_score, KFold \n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    chunktag = sent[i][2]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isalpha()': word.isalpha(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "        'chunktag': chunktag,\n",
    "        'chunktag[:2]': chunktag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][1]\n",
    "        chunktag1 = sent[i - 1][2]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:word.isalpha()': word1.isalpha(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "            '-1:chunktag': chunktag1,\n",
    "            '-1:chunktag[:2]': chunktag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    if i > 1:\n",
    "        word2 = sent[i - 2][0]\n",
    "        postag2 = sent[i - 2][1]\n",
    "        chunktag2 = sent[i - 2][2]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word.istitle()': word2.istitle(),\n",
    "            '-2:word.isupper()': word2.isupper(),\n",
    "            '-2:word.isalpha()': word2.isalpha(),\n",
    "            '-2:postag': postag2,\n",
    "            '-2:postag[:2]': postag2[:2],\n",
    "            '-2:chunktag': chunktag2,\n",
    "            '-2:chunktag[:2]': chunktag2[:2],\n",
    "        })\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = sent[i + 1][1]\n",
    "        chunktag1 = sent[i + 1][2]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:word.isalpha()': word1.isalpha(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "            '+1:chunktag': chunktag1,\n",
    "            '+1:chunktag[:2]': chunktag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "    if i < len(sent) - 2:\n",
    "        word2 = sent[i + 2][0]\n",
    "        postag2 = sent[i + 2][1]\n",
    "        chunktag2 = sent[i + 2][2]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.istitle()': word2.istitle(),\n",
    "            '+2:word.isupper()': word2.isupper(),\n",
    "            '+2:word.isalpha()': word2.isalpha(),\n",
    "            '+2:postag': postag2,\n",
    "            '+2:postag[:2]': postag2[:2],\n",
    "            '+2:chunktag': chunktag2,\n",
    "            '+2:chunktag[:2]': chunktag2[:2],\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_history(docs):\n",
    "    for doc in docs:\n",
    "        all_tokens = []\n",
    "        for sent in doc:\n",
    "            all_tokens += sent\n",
    "        for i in range(0, len(all_tokens), 1):\n",
    "            for j in range(i - 1, max(0, i - 1000), -1):\n",
    "                if all_tokens[i]['word.lower()'] == all_tokens[j]['word.lower()']:\n",
    "                    all_tokens[i].update({key + '_history': value \n",
    "                                          for key, value in all_tokens[j].items() if key != 'word.lower()'})\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-level predictions using 10-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "\n",
    "x_train_docs, y_train_docs = utils.docs_from_dataset('./dataset', 'eng.train.txt', \n",
    "                                                     ('words', 'pos', 'chunk', 'ne'), \n",
    "                                                     ['words', 'pos', 'chunk'], sent2features)\n",
    "\n",
    "add_history(x_train_docs)\n",
    "x_train_docs = np.array(x_train_docs)\n",
    "y_train_docs = np.array(y_train_docs)\n",
    "\n",
    "if os.path.exists('./prepared_data/y_train_docs_predicted.npy'):\n",
    "    y_train_docs_predicted = np.load('./prepared_data/y_train_docs_predicted.npy')\n",
    "else:\n",
    "    y_train_docs_predicted = [0 for i in range(len(y_train_docs))]\n",
    "\n",
    "    for train_ids, test_ids in kf.split(x_train_docs):      \n",
    "        crf_first_train = sc.SequenceClassifier(cls='CRF')\n",
    "        crf_first_train.fit(x_train_docs[train_ids], y_train_docs[train_ids])                                            \n",
    "        y_predicted_docs = crf_first_train.predict(x_train_docs[test_ids])\n",
    "        i = 0\n",
    "        for index in test_ids:\n",
    "            y_train_docs_predicted[index] = y_predicted_docs[i]\n",
    "            i += 1\n",
    "    np.save('./prepared_data/y_train_docs_predicted.npy', y_train_docs_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-majority features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_features(x_train_docs, y_train_docs_predicted):\n",
    "    tokens_corp = dict()\n",
    "\n",
    "    for x_doc, y_doc in zip(x_train_docs, y_train_docs_predicted):\n",
    "        tokens_doc = dict()\n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            for x_token, y_token in zip(x_sent, y_sent):\n",
    "                token = x_token['word.lower()']\n",
    "                # prefixes\n",
    "                if y_token != 'O':\n",
    "                    y_token = y_token[2:] \n",
    "                if token in tokens_corp:\n",
    "                    tokens_corp[token].append(y_token)\n",
    "                else:\n",
    "                    tokens_corp[token]= [y_token]\n",
    "                if token in tokens_doc:\n",
    "                    tokens_doc[token].append(y_token)\n",
    "                else:\n",
    "                    tokens_doc[token]= [y_token]\n",
    "\n",
    "        for key in tokens_doc.keys():\n",
    "            tokens_doc[key] = Counter(tokens_doc[key])\n",
    "        for x_sent in x_doc:\n",
    "            for x_token in x_sent:\n",
    "                x_token['doc_token_maj'] = tokens_doc[x_token['word.lower()']].most_common(1)[0][0]\n",
    "\n",
    "    for key in tokens_corp.keys():\n",
    "        tokens_corp[key] = Counter(tokens_corp[key])   \n",
    "\n",
    "    for x_doc in x_train_docs:          \n",
    "        for x_sent in x_doc:\n",
    "            for x_token in x_sent:\n",
    "                x_token['corp_token_maj'] = tokens_corp[x_token['word.lower()']].most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity-majority features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entity_features(x_train_docs, y_train_docs_predicted):\n",
    "    entities_corp = dict()\n",
    "    current_entity = ''\n",
    "\n",
    "    for x_doc, y_doc in zip(x_train_docs, y_train_docs_predicted):\n",
    "        entities_doc = dict()\n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            current_indexes = []\n",
    "            current_entity = ''\n",
    "            for x_token, y_token in zip(x_sent, y_sent):\n",
    "                token = x_token['word.lower()']\n",
    "                if y_token[0] == 'S':\n",
    "                    current_entity = token\n",
    "                    if current_entity in entities_doc:\n",
    "                        entities_doc[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        entities_doc[current_entity] = [y_token[2:]]\n",
    "                    if current_entity in entities_corp:\n",
    "                        entities_corp[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        entities_corp[current_entity] = [y_token[2:]]\n",
    "                if y_token[0] == 'B':\n",
    "                    current_entity = token\n",
    "                if y_token[0] == 'I':\n",
    "                    current_entity += ' ' + token\n",
    "                if y_token[0] == 'E':\n",
    "                    current_entity += ' ' + token\n",
    "                    if current_entity in entities_doc:\n",
    "                        entities_doc[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        entities_doc[current_entity] = [y_token[2:]]\n",
    "                    if current_entity in entities_corp:\n",
    "                        entities_corp[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        entities_corp[current_entity] = [y_token[2:]]\n",
    "                    current_entity = ''\n",
    "                if y_token == 'O':\n",
    "                    current_entity = ''\n",
    "        for key in entities_doc.keys():\n",
    "            entities_doc[key] = Counter(entities_doc[key])\n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            current_indexes = []\n",
    "            current_entity = ''\n",
    "            for i in range(len(x_sent)):\n",
    "                if y_sent[i][0] == 'S':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    for j in current_indexes:\n",
    "                        x_sent[j]['doc_entity_maj'] = entities_doc[current_entity].most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i][0] == 'B':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                if y_sent[i][0] == 'I':\n",
    "                    current_entity = current_entity + ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                if y_sent[i][0] == 'E':\n",
    "                    current_entity = current_entity + ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                    if current_entity in entities_doc:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['doc_entity_maj'] = entities_doc[current_entity].most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i] == 'O':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    if current_entity in entities_doc:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['corp_entity_maj'] = entities_doc[current_entity].most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "\n",
    "\n",
    "    for key in entities_corp.keys():\n",
    "        entities_corp[key] = Counter(entities_corp[key])   \n",
    "\n",
    "    for x_doc, y_doc in zip(x_train_docs, y_train_docs_predicted):          \n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            current_indexes = []\n",
    "            for i in range(len(x_sent)):\n",
    "                if y_sent[i][0] == 'S':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    for j in current_indexes:\n",
    "                        x_sent[j]['corp_entity_maj'] = entities_corp[current_entity].most_common(1)[0][0]\n",
    "                    current_indexes = []\n",
    "                    current_entity = ''\n",
    "                if y_sent[i][0] == 'B':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                if y_sent[i][0] == 'I':\n",
    "                    current_entity += ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                if y_sent[i][0] == 'E':\n",
    "                    current_entity += ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                    if current_entity in entities_corp:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['corp_entity_maj'] = entities_corp[current_entity].most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i] == 'O':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    if current_entity in entities_corp:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['corp_entity_maj'] = entities_corp[current_entity].most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superentity-majority features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_super_features(x_train_docs, y_train_docs_predicted):\n",
    "    super_corp = dict()\n",
    "    current_entity = ''\n",
    "\n",
    "    for x_doc, y_doc in zip(x_train_docs, y_train_docs_predicted):\n",
    "        super_doc = dict()\n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            for x_token, y_token in zip(x_sent, y_sent):\n",
    "                token = x_token['word.lower()']\n",
    "                if y_token[0] == 'S':\n",
    "                    current_entity = token\n",
    "                    if current_entity in super_doc:\n",
    "                        super_doc[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        super_doc[current_entity] = [y_token[2:]]\n",
    "                    if current_entity in super_corp:\n",
    "                        super_corp[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        super_corp[current_entity] = [y_token[2:]]\n",
    "                if y_token[0] == 'B':\n",
    "                    current_entity = token\n",
    "                if y_token[0] == 'I':\n",
    "                    current_entity += ' ' + token\n",
    "                if y_token[0] == 'E':\n",
    "                    current_entity += ' ' + token\n",
    "                    if current_entity in super_doc:\n",
    "                        super_doc[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        super_doc[current_entity] = [y_token[2:]]\n",
    "                    if current_entity in super_corp:\n",
    "                        super_corp[current_entity].append(y_token[2:])\n",
    "                    else:\n",
    "                        super_corp[current_entity] = [y_token[2:]]\n",
    "                    current_entity = ''\n",
    "                if y_token == 'O':\n",
    "                    current_entity = ''\n",
    "        for key in super_doc.keys():\n",
    "            super_doc[key] = Counter(super_doc[key])\n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            current_indexes = []\n",
    "            for i in range(len(x_sent)):\n",
    "                if y_sent[i][0] == 'S':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    current_counter = Counter()\n",
    "                    for key in super_doc.keys():\n",
    "                        if current_entity in key and len(current_entity) != len(key):\n",
    "                            current_counter += super_doc[key]\n",
    "                    if len(current_counter.keys()) != 0:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['doc_super_maj'] = current_counter.most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i][0] == 'B':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                if y_sent[i][0] == 'I':\n",
    "                    current_entity += ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                if y_sent[i][0] == 'E':\n",
    "                    current_entity += ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                    current_counter = Counter()\n",
    "                    for key in super_doc.keys():\n",
    "                        if current_entity in key and len(current_entity) != len(key):\n",
    "                            current_counter += super_doc[key]\n",
    "                    if len(current_counter.keys()) != 0:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['doc_super_maj'] = current_counter.most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i] == 'O':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    current_counter = Counter()\n",
    "                    for key in super_doc.keys():\n",
    "                        if current_entity in key and len(current_entity) != len(key):\n",
    "                            current_counter += super_doc[key]\n",
    "                    if len(current_counter.keys()) != 0:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['doc_super_maj'] = current_counter.most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "\n",
    "    for key in super_corp.keys():\n",
    "        super_corp[key] = Counter(super_corp[key])\n",
    "    for x_doc, y_doc in zip(x_train_docs, y_train_docs_predicted):\n",
    "        for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "            current_indexes = []\n",
    "            for i in range(len(x_sent)):\n",
    "                if y_sent[i][0] == 'S':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    current_counter = Counter()\n",
    "                    for key in super_corp.keys():\n",
    "                        if current_entity in key and len(current_entity) != len(key):\n",
    "                            current_counter += super_corp[key]\n",
    "                    if len(current_counter.keys()) != 0:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['corp_super_maj'] = current_counter.most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i][0] == 'B':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                if y_sent[i][0] == 'I':\n",
    "                    current_entity += ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                if y_sent[i][0] == 'E':\n",
    "                    current_entity += ' ' + x_sent[i]['word.lower()']\n",
    "                    current_indexes.append(i)\n",
    "                    current_counter = Counter()\n",
    "                    for key in super_corp.keys():\n",
    "                        if current_entity in key and len(current_entity) != len(key):\n",
    "                            current_counter += super_corp[key]\n",
    "                    if len(current_counter.keys()) != 0:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['corp_super_maj'] = current_counter.most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []\n",
    "                if y_sent[i] == 'O':\n",
    "                    current_entity = x_sent[i]['word.lower()']\n",
    "                    current_indexes = [i]\n",
    "                    current_counter = Counter()\n",
    "                    for key in super_corp.keys():\n",
    "                        if current_entity in key and len(current_entity) != len(key):\n",
    "                            current_counter += super_corp[key]\n",
    "                    if len(current_counter.keys()) != 0:\n",
    "                        for j in current_indexes:\n",
    "                            x_sent[j]['corp_super_maj'] = current_counter.most_common(1)[0][0]\n",
    "                    current_entity = ''\n",
    "                    current_indexes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction CRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifier(algorithm='lbfgs', all_possible_states=None,\n",
       "          all_possible_transitions=True, averaging=None, c=None, c1=0.1,\n",
       "          c2=0.1, calibration_candidates=None, calibration_eta=None,\n",
       "          calibration_max_trials=None, calibration_rate=None,\n",
       "          calibration_samples=None, cls='CRF', delta=None, epsilon=None,\n",
       "          error_sensitive=None, gamma=None, keep_tempfiles=None,\n",
       "          linesearch=None, max_iterations=100, max_linesearch=None,\n",
       "          min_freq=None, model_filename=None, num_memories=None,\n",
       "          pa_type=None, period=None, trainer_cls=None, variance=None,\n",
       "          verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_first_train = sc.SequenceClassifier(cls='CRF')\n",
    "crf_first_train.fit(x_train_docs, y_train_docs)\n",
    "\n",
    "add_token_features(x_train_docs, y_train_docs_predicted)\n",
    "add_entity_features(x_train_docs, y_train_docs_predicted)\n",
    "add_super_features(x_train_docs, y_train_docs_predicted)\n",
    "\n",
    "crf_second_train = sc.SequenceClassifier(cls='CRF')\n",
    "crf_second_train.fit(x_train_docs, y_train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты EXTENDED + HISTORY + KRISHNAN_MANNING\n",
      "label    precision    recall    f1-score\n",
      "\n",
      "PER      0.9132       0.9420    0.9274\n",
      "ORG      0.7586       0.7880    0.7730\n",
      "LOC      0.8778       0.8283    0.8524\n",
      "MISC     0.8363       0.7417    0.7862\n",
      "\n",
      "total    0.8464       0.8377    0.8420\n"
     ]
    }
   ],
   "source": [
    "x_testb_docs, y_testb_docs = utils.docs_from_dataset('./dataset', 'eng.testb.test.txt', \n",
    "                                                     ('words', 'pos', 'chunk', 'ne'), \n",
    "                                                     ['words', 'pos', 'chunk'], sent2features)\n",
    "add_history(x_testb_docs)\n",
    "y_testb_docs_predicted = crf_first_train.predict(x_testb_docs)\n",
    "add_token_features(x_testb_docs, y_testb_docs_predicted)\n",
    "add_entity_features(x_testb_docs, y_testb_docs_predicted)\n",
    "add_super_features(x_testb_docs, y_testb_docs_predicted)\n",
    "print('Результаты EXTENDED + HISTORY + KRISHNAN_MANNING')\n",
    "crf_second_train.get_full_score(x_testb_docs, y_testb_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
