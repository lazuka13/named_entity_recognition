{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим, какие признаки генерируются в случае с использованием истории и без нее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Без истории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import features\n",
    "import reader\n",
    "import os\n",
    "\n",
    "from utils import docs_from_dataset_tokens, xdocs_from_x_dataset\n",
    "from classifiers.token_classifier import TokenClassifier\n",
    "\n",
    "dataset_train = reader.DataReader(\n",
    "    './dataset',\n",
    "    fileids='eng.train.txt',\n",
    "    columntypes=('words', 'pos', 'chunk', 'ne'))\n",
    "dataset_testb = reader.DataReader(\n",
    "    './dataset',\n",
    "    fileids='eng.testb.test.txt',\n",
    "    columntypes=('words', 'pos', 'chunk', 'ne'))\n",
    "gen = features.Generator(\n",
    "    columntypes=('words', 'pos', 'chunk'),\n",
    "    context_len=2,\n",
    "    language='en',\n",
    "    rare_count=5,\n",
    "    min_weight=0.975,\n",
    "    rewrite=True,\n",
    "    history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаков в исходном виде: 30\n",
      "Обработано 203621 токенов!\n",
      "Признаков осталось: 6451\n",
      "\n",
      "label    precision    recall    f1-score\n",
      "\n",
      "PER      0.7520       0.8435    0.7951\n",
      "ORG      0.7019       0.6988    0.7003\n",
      "LOC      0.7869       0.8214    0.8038\n",
      "MISC     0.7393       0.6749    0.7056\n",
      "\n",
      "total    0.7465       0.7727    0.7594\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_docs_train, y_docs_train = docs_from_dataset_tokens(dataset_train)\n",
    "x_feat_train = gen.fit_generate(x_docs_train, y_docs_train,\n",
    "                                \"./prepared_data/conll_train.npz\")\n",
    "x_docs_feat_train = xdocs_from_x_dataset(x_feat_train, dataset_train)\n",
    "\n",
    "x_docs_testb, y_docs_testb = docs_from_dataset_tokens(dataset_testb)\n",
    "x_feat_testb = gen.generate(x_docs_testb, \"./prepared_data/conll_testb.npz\")\n",
    "x_docs_feat_testb = xdocs_from_x_dataset(x_feat_testb, dataset_testb)\n",
    "\n",
    "clf = TokenClassifier(\n",
    "    cls='XGBClassifier',\n",
    "    learning_rate=0.3,\n",
    "    max_depth=14,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5)\n",
    "clf.fit(x_docs_feat_train, y_docs_train)\n",
    "\n",
    "print()\n",
    "\n",
    "clf.get_full_score(x_docs_feat_testb, y_docs_testb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "features_classes = gen.features_classes_\n",
    "\n",
    "booster = clf.obj.get_booster()\n",
    "\n",
    "features_counts = booster.get_score(importance_type='weight')\n",
    "features_gains = booster.get_score(importance_type='gain')\n",
    "\n",
    "all_features_counts = [\n",
    "    features_counts.get(f, 0.) for f in booster.feature_names\n",
    "]\n",
    "\n",
    "all_features_gains = [\n",
    "    features_gains.get(f, 0.) for f in booster.feature_names\n",
    "]\n",
    "\n",
    "all_features_gains = numpy.array(all_features_gains, dtype=numpy.float32)\n",
    "all_features_counts = numpy.array(all_features_counts, dtype=numpy.float32)\n",
    "\n",
    "with open('out.txt', 'w+', encoding='utf-8') as file:\n",
    "    for name, weight, gain, importance in sorted(\n",
    "            zip(features_classes, all_features_counts, all_features_gains,\n",
    "                clf.obj.feature_importances_),\n",
    "            key=lambda x: -x[1]):\n",
    "        file.write(f'{name:40} | weight: {weight:20} | gain: {gain:20} | importance: {importance:16.10f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) С историей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hist = features.Generator(\n",
    "    columntypes=('words', 'pos', 'chunk'),\n",
    "    context_len=2,\n",
    "    language='en',\n",
    "    rare_count=5,\n",
    "    min_weight=0.975,\n",
    "    rewrite=True,\n",
    "    history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаков в исходном виде: 30\n",
      "Обработано 203621 токенов!\n",
      "9974 токенов имеют историю в рамках документа!\n",
      "Признаков осталось: 6818\n",
      "\n",
      "label    precision    recall    f1-score\n",
      "\n",
      "PER      0.7497       0.8371    0.7910\n",
      "ORG      0.7010       0.7062    0.7036\n",
      "LOC      0.7976       0.8246    0.8108\n",
      "MISC     0.7264       0.6821    0.7036\n",
      "\n",
      "total    0.7469       0.7749    0.7607\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_docs_train_hist, y_docs_train_hist = docs_from_dataset_tokens(dataset_train)\n",
    "x_feat_train_hist = gen_hist.fit_generate(\n",
    "    x_docs_train_hist, y_docs_train_hist,\n",
    "    \"./prepared_data/conll_train_history.npz\")\n",
    "x_docs_feat_train_hist = xdocs_from_x_dataset(x_feat_train_hist, dataset_train)\n",
    "\n",
    "x_docs_testb_hist, y_docs_testb_hist = docs_from_dataset_tokens(dataset_testb)\n",
    "x_feat_testb_hist = gen_hist.generate(x_docs_testb_hist, \"./prepared_data/conll_testb_history.npz\")\n",
    "x_docs_feat_testb_hist = xdocs_from_x_dataset(x_feat_testb_hist, dataset_testb)\n",
    "\n",
    "clf_hist = TokenClassifier(\n",
    "    cls='XGBClassifier',\n",
    "    learning_rate=0.3,\n",
    "    max_depth=14,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5)\n",
    "clf_hist.fit(x_docs_feat_train_hist, y_docs_train_hist)\n",
    "\n",
    "print()\n",
    "\n",
    "clf_hist.get_full_score(x_docs_feat_testb_hist, y_docs_testb_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "features_classes = gen_hist.features_classes_\n",
    "\n",
    "booster = clf_hist.obj.get_booster()\n",
    "\n",
    "features_counts = booster.get_score(importance_type='weight')\n",
    "features_gains = booster.get_score(importance_type='gain')\n",
    "\n",
    "all_features_counts = [\n",
    "    features_counts.get(f, 0.) for f in booster.feature_names\n",
    "]\n",
    "\n",
    "all_features_gains = [\n",
    "    features_gains.get(f, 0.) for f in booster.feature_names\n",
    "]\n",
    "\n",
    "all_features_gains = numpy.array(all_features_gains, dtype=numpy.float32)\n",
    "all_features_counts = numpy.array(all_features_counts, dtype=numpy.float32)\n",
    "\n",
    "with open('out_hist.txt', 'w+', encoding='utf-8') as file:\n",
    "    for name, weight, gain, importance in sorted(\n",
    "            zip(features_classes, all_features_counts, all_features_gains,\n",
    "                clf_hist.obj.feature_importances_),\n",
    "            key=lambda x: -x[1]):\n",
    "        file.write(f'{name:40} | weight: {weight:20} | gain: {gain:20} | importance: {importance:16.10f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
