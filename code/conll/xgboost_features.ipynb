{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим, какие признаки генерируются в случае с использованием истории и без нее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Без истории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import features\n",
    "import reader\n",
    "import os\n",
    "\n",
    "from classifiers.token_classifier import TokenClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = reader.DataReader(\n",
    "    './dataset',\n",
    "    fileids='eng.train.txt',\n",
    "    columntypes=('words', 'pos', 'chunk', 'ne'))\n",
    "dataset_testb = reader.DataReader(\n",
    "    './dataset',\n",
    "    fileids='eng.testb.test.txt',\n",
    "    columntypes=('words', 'pos', 'chunk', 'ne'))\n",
    "gen = features.Generator(\n",
    "    columntypes=('words', 'pos', 'chunk'),\n",
    "    context_len=2,\n",
    "    language='en',\n",
    "    rare_count=5,\n",
    "    min_weight=0.95,\n",
    "    rewrite=True,\n",
    "    history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_from_dataset_tokens(dataset, tags=['words', 'pos', 'chunk']):\n",
    "    y = [el[1] for el in dataset.get_ne()]\n",
    "    x = dataset.get_tags(tags=tags)\n",
    "\n",
    "    x_sent, y_sent = [], []\n",
    "    index = 0\n",
    "    for sent in dataset.sents():\n",
    "        length = len(sent)\n",
    "        if length == 0:\n",
    "            continue\n",
    "        x_sent.append(x[index:index + length])\n",
    "        y_sent.append(y[index:index + length])\n",
    "        index += length\n",
    "    x_docs, y_docs = [], []\n",
    "    index = 0\n",
    "    for doc in dataset.docs():\n",
    "        length = len(doc)\n",
    "        if length == 0:\n",
    "            continue\n",
    "        x_docs.append(x_sent[index:index + length])\n",
    "        y_docs.append(y_sent[index:index + length])\n",
    "        index += length\n",
    "    return x_docs, y_docs\n",
    "\n",
    "\n",
    "def xdocs_from_x_dataset(x, dataset):\n",
    "    x_sent = []\n",
    "    index = 0\n",
    "    for sent in dataset.sents():\n",
    "        length = len(sent)\n",
    "        if length == 0:\n",
    "            continue\n",
    "        x_sent.append(x[index:index + length])\n",
    "        index += length\n",
    "    x_docs = []\n",
    "    index = 0\n",
    "    for doc in dataset.docs():\n",
    "        length = len(doc)\n",
    "        if length == 0:\n",
    "            continue\n",
    "        x_docs.append(x_sent[index:index + length])\n",
    "        index += length\n",
    "    return x_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаков в исходном виде: (203621, 30)\n",
      "Признаков осталось: 4291\n",
      "Wall time: 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_docs_train, y_docs_train = docs_from_dataset_tokens(dataset_train)\n",
    "x_feat_train = gen.fit_generate(x_docs_train, y_docs_train,\n",
    "                                \"./prepared_data/conll_train.npz\")\n",
    "x_docs_feat_train = xdocs_from_x_dataset(x_feat_train, dataset_train)\n",
    "\n",
    "x_docs_testb, y_docs_testb = docs_from_dataset_tokens(dataset_testb)\n",
    "x_feat_testb = gen.generate(x_docs_testb, \"./prepared_data/conll_testb.npz\")\n",
    "x_docs_feat_testb = xdocs_from_x_dataset(x_feat_testb, dataset_testb)\n",
    "\n",
    "clf = TokenClassifier(\n",
    "    cls='XGBClassifier',\n",
    "    learning_rate=0.3,\n",
    "    max_depth=14,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5)\n",
    "clf.fit(x_docs_feat_train, y_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    precision    recall    f1-score\n",
      "\n",
      "PER      0.7530       0.8364    0.7926\n",
      "ORG      0.7090       0.7099    0.7095\n",
      "LOC      0.8004       0.8239    0.8120\n",
      "MISC     0.7386       0.6604    0.6973\n",
      "\n",
      "total    0.7525       0.7729    0.7626\n"
     ]
    }
   ],
   "source": [
    "clf.get_full_score(x_docs_feat_testb, y_docs_testb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "features_classes = gen.features_classes_\n",
    "\n",
    "booster = clf.obj.get_booster()\n",
    "features_counts = booster.get_score(importance_type='weight')\n",
    "\n",
    "all_features_counts = [\n",
    "    features_counts.get(f, 0.) for f in booster.feature_names\n",
    "]\n",
    "all_features_counts = numpy.array(all_features_counts, dtype=numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'w+', encoding='utf-8') as file:\n",
    "    for name, gain, weight in sorted(\n",
    "            zip(features_classes, all_features_counts,\n",
    "                clf.obj.feature_importances_),\n",
    "            key=lambda x: -x[1]):\n",
    "        file.write(f'{name:15}{gain:12}{weight:14.8f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) С историей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hist = features.Generator(\n",
    "    columntypes=('words', 'pos', 'chunk'),\n",
    "    context_len=2,\n",
    "    language='en',\n",
    "    rare_count=5,\n",
    "    min_weight=0.95,\n",
    "    rewrite=True,\n",
    "    history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаков в исходном виде: (203621, 60)\n",
      "Признаков осталось: 4356\n",
      "Wall time: 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_docs_train_hist, y_docs_train_hist = docs_from_dataset_tokens(dataset_train)\n",
    "x_feat_train_hist = gen_hist.fit_generate(\n",
    "    x_docs_train_hist, y_docs_train_hist,\n",
    "    \"./prepared_data/conll_train_history.npz\")\n",
    "x_docs_feat_train_hist = xdocs_from_x_dataset(x_feat_train_hist, dataset_train)\n",
    "\n",
    "x_docs_testb_hist, y_docs_testb_hist = docs_from_dataset_tokens(dataset_testb)\n",
    "x_feat_testb_hist = gen_hist.generate(\n",
    "    x_docs_testb, \"./prepared_data/conll_testb_history.npz\")\n",
    "x_docs_feat_testb_hist = xdocs_from_x_dataset(x_feat_testb_hist, dataset_testb)\n",
    "\n",
    "clf_hist = TokenClassifier(\n",
    "    cls='XGBClassifier',\n",
    "    learning_rate=0.3,\n",
    "    max_depth=14,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5)\n",
    "clf_hist.fit(x_docs_feat_train_hist, y_docs_train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    precision    recall    f1-score\n",
      "\n",
      "PER      0.7542       0.8358    0.7929\n",
      "ORG      0.6932       0.7056    0.6994\n",
      "LOC      0.7934       0.8252    0.8090\n",
      "MISC     0.7356       0.6662    0.6992\n",
      "\n",
      "total    0.7458       0.7726    0.7589\n"
     ]
    }
   ],
   "source": [
    "clf_hist.get_full_score(x_docs_feat_testb_hist, y_docs_testb_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "features_classes = gen_hist.features_classes_\n",
    "\n",
    "booster = clf_hist.obj.get_booster()\n",
    "features_counts = booster.get_score(importance_type='weight')\n",
    "\n",
    "all_features_counts = [\n",
    "    features_counts.get(f, 0.) for f in booster.feature_names\n",
    "]\n",
    "all_features_counts = numpy.array(all_features_counts, dtype=numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out_hist.txt', 'w+', encoding='utf-8') as file:\n",
    "    for name, gain, weight in sorted(\n",
    "            zip(features_classes, all_features_counts,\n",
    "                clf.obj.feature_importances_),\n",
    "            key=lambda x: -x[1]):\n",
    "        file.write(f'{name:15}{gain:12}{weight:14.8f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
